# -*- coding: utf-8 -*-
"""SentimentAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bond6-hfZS4iAViWGgOTTBn7enBLbE-3

# **IMPORTING LIBRARIES**

---
"""

import re
import nltk
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
from wordcloud import WordCloud
from tensorflow.keras import layers, models, losses, Sequential, optimizers, metrics

pip install seaborn -U

"""# **TWITTER DATASET**

---



"""

tweets_data = pd.read_csv("Tweets.csv")

tweets_data.sample(10)

tweets_data.info()

tweets_data.isnull().sum()

tweets_data = tweets_data.dropna()

tweets_data.isnull().sum()

tweets_data.duplicated().sum()

tweets_data = tweets_data.drop(columns = ["selected_text", "textID"])

tweets_data.sample(10)

"""# **EXPLORATORY ANALYSIS**

---



"""

tweets_data["sentiment"].value_counts()

import seaborn as sns

sns.countplot(data = tweets_data, x = "sentiment")

tweets_data["word_count"] = tweets_data["text"].apply(lambda x: len(str(x).split(" ")))

tweets_data[["text", "word_count"]].sample(10)

tweets_data.groupby("sentiment")["word_count"].mean()

tweets_data.groupby("sentiment")["word_count"].median()

"""
* **Mean > Median**: Data is skewed to the right (positive
skew).
* **Mean < Median**: Data is skewed to the left (negative skew).
* **Mean â‰ˆ Median**: Data is roughly symmetrical.



"""

sns.histplot(data = tweets_data, x = "word_count", hue = "sentiment", kde = True)

"""HISTOGRAM FOR WORD COUNT IN EACH SENTIMENT
* **How word count varies across sentiments**
* **The overall distribution of word counts**
"""

sns.boxplot(data = tweets_data, y = "word_count", hue = "sentiment", showmeans = True)

"""BOX PLOT FOR THE SENTIMENTS
*   **Central Tendency**
*   **Spread & Variability**
*   **Outliers**
*   **Differences between sentiments**


"""

from nltk.corpus import stopwords

nltk.download("stopwords")

stop_words = stopwords.words("english")

print(stop_words)

"""# **DATA CLEANING**

---


"""

import re
import string


def custom_standardization(input_data):

    # Convert to lowercase
    lowercase = input_data.lower()

    # Remove URLs
    stripped_urls = re.sub(r"https?://\S+|www.\S+", "",lowercase)

    # Remove email addresses
    stripped_symbol = re.sub(r"\S*@\S*\s?", "",stripped_urls)

    # Remove text in angular brackets (usually HTML tags)
    stripped_brackets = re.sub(r"<.*?>+", "",stripped_symbol)

    # Remove any square brackets and leave the text within square brackets
    stripped_brackets = re.sub(r"\[|\]", "",stripped_brackets)
    # Matches alphanumeric characters with digits and remove those
    stripped_digits = re.sub(r"\w*\d\w*", "",stripped_brackets)
    # Remove stopwords
    stripped_stopwords = re.sub( r"\b(?:{})\b".format("|".join(stop_words)), "", stripped_digits) # Use re.sub() for substitution

    # Replace multiple whitespaces with a single whitespace
    stripped_whitespace_chars = re.sub(r"\s+", " ",stripped_stopwords)

    # Remove non-alphabet characters
    return re.sub(r"[^a-zA-Z\s]+" ,"", stripped_whitespace_chars)

tweets_data["text"][5]

custom_standardization(tweets_data["text"][5])

tweets_data["text"][20]

custom_standardization(tweets_data["text"][20])

tweets_data["cleaned_text"] = tweets_data["text"].apply(custom_standardization)

tweets_data

# Extract the text from positive sentiment tweets
positive_tweets = tweets_data[tweets_data["sentiment"] == "positive"]["cleaned_text"]

# Concatenate all the positive sentiment tweets into a single string
positive_text = " ".join(positive_tweets)

# Create a WordCloud object
wordcloud = WordCloud(width = 800, height = 400, background_color = "white",colormap="Greens").generate(positive_text)

# Plot the WordCloud
plt.figure(figsize = (10, 6))
plt.imshow(wordcloud, interpolation = "bilinear")
plt.axis("off")
plt.title("Word Cloud - Positive Sentiment")
plt.show()

# Extract the text from negative sentiment tweets
negative_tweets = tweets_data[tweets_data["sentiment"] == "negative"]["cleaned_text"]

# Concatenate all the negative sentiment tweets into a single string
negative_text = " ".join(negative_tweets)

# Create a WordCloud object
wordcloud = WordCloud(width = 800, height = 400, background_color = "white",colormap="Reds").generate(negative_text)

# Plot the WordCloud
plt.figure(figsize = (10, 6))
plt.imshow(wordcloud, interpolation = "bilinear")
plt.axis("off")
plt.title("Word Cloud - Negative Sentiment")
plt.show()

# Extract the text from neutral sentiment tweets
neutral_tweets = tweets_data[tweets_data["sentiment"] == "neutral"]["cleaned_text"]

# Concatenate all the neutral sentiment tweets into a single string
neutral_tweets = " ".join(neutral_tweets)

# Create a WordCloud object
wordcloud = WordCloud(width = 800, height = 400, background_color = "white").generate(neutral_tweets)

# Plot the WordCloud
plt.figure(figsize = (10, 6))
plt.imshow(wordcloud, interpolation = "bilinear")
plt.axis("off")
plt.title("Word Cloud - Neutral Sentiment")
plt.show()

tweets_data["sentiment"] = tweets_data["sentiment"].replace({"negative": 0, "neutral": 1, "positive": 2})

tweets_data.sample(10)

"""# **SPLITTING THE DATASET**

---


"""

from sklearn.model_selection import train_test_split

X_train, X_test = train_test_split(tweets_data, test_size = 0.2, stratify = tweets_data["sentiment"], random_state = 123)
X_train, X_val = train_test_split(X_train, test_size = 0.1, stratify = X_train["sentiment"], random_state = 123)

X_train.shape, X_val.shape, X_test.shape

"""* **Training: 72%**
* **Validation: 8%**
* **Testing: 20%**
"""

X_train["sentiment"].value_counts()

X_val["sentiment"].value_counts()

X_test["sentiment"].value_counts()

texts = tweets_data['text'].fillna('')
labels = tweets_data['sentiment']

"""## **TEXT VECTORIZATION**-CONVERING THE TEXTS INTO VECTORS
---

* BAG OF WORDS
* TF-IDF
"""

# Bag of Words vectorization
bow_vectorizer = CountVectorizer()
X_bow = bow_vectorizer.fit_transform(texts)

# Analyze the most frequent words
def plot_top_words(vectorizer, X, title):
    sum_words = X.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)[:20]
    words, freqs = zip(*words_freq)
    plt.figure(figsize=(10,8))
    plt.barh(words, freqs)
    plt.gca().invert_yaxis()
    plt.title(title)
    plt.show()

plot_top_words(bow_vectorizer, X_bow, "Top 20 words in Bag of Words")

texts = tweets_data['cleaned_text'].fillna('')

# TF-IDF vectorization
tfidf_vectorizer = TfidfVectorizer(smooth_idf=True,max_df=0.9)
X_tfidf = tfidf_vectorizer.fit_transform(texts)

plot_top_words(tfidf_vectorizer, X_tfidf, "Top 20 words in TF-IDF")

"""## SMALLER PORTION OF THE DATASET FOR GRIDSEARCH"""

# Train-test split
X_train_bow, X_test_bow, y_train, y_test = train_test_split(X_bow, labels, test_size=0.2, random_state=42)
X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, labels, test_size=0.2, random_state=42)

# Sample a smaller portion of the training data
X_train_bow_small = X_train_bow[:int(0.2 * X_train_bow.shape[0])]
y_train_small = y_train[:int(0.2 * y_train.shape[0])]

# Use the smaller dataset for GridSearchCV
X_train_tfidf_small = X_train_tfidf[:int(0.2 * X_train_tfidf.shape[0])]
y_train_small = y_train[:int(0.2 * y_train.shape[0])]

from sklearn import svm
svm_bow = svm.SVC()

"""**SVM-SUPPORT VECTOR MACHINE PARAMETERS**


*   Regularization Parameter(C)
*   Kernel


"""

# Cross-validation and hyperparameter tuning for SVM
svm_params = {'C': [0.5, 1, 5, 10], 'kernel': ['linear', 'rbf']}

svm_bow = GridSearchCV(SVC(), svm_params, cv=5)
svm_bow.fit(X_train_bow_small, y_train_small)
print("Best SVM params for BoW:", svm_bow.best_params_)
svm_tfidf = GridSearchCV(SVC(), svm_params, cv=5)
svm_tfidf.fit(X_train_tfidf_small, y_train_small)
print("Best SVM params for TF-IDF:", svm_tfidf.best_params_)

"""**NAIVE BAYES PARAMETER**


*   Alpha



"""

# Cross-validation and hyperparameter tuning for Naive Bayes
nb_params = {'alpha': [0.01, 0.1, 1, 10]}
nb_bow = GridSearchCV(MultinomialNB(), nb_params, cv=5)
nb_bow.fit(X_train_bow_small, y_train_small)
print("Best Naive Bayes params for BoW:", nb_bow.best_params_)
nb_tfidf = GridSearchCV(MultinomialNB(), nb_params, cv=5)
nb_tfidf.fit(X_train_tfidf_small, y_train_small)
print("Best Naive Bayes params for TF-IDF:", nb_tfidf.best_params_)

# Evaluate models
def evaluate_model(model, X_test, y_test, title):
    y_pred = model.predict(X_test)
    print(f"{title} Accuracy:", accuracy_score(y_test, y_pred))
    print(f"{title} Classification Report:\n", classification_report(y_test, y_pred))
    conf_matrix = confusion_matrix(y_test, y_pred)
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues")
    plt.title(f"{title} Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

# SVM with BoW
evaluate_model(svm_bow.best_estimator_, X_test_bow, y_test, "SVM with BoW")

# Naive Bayes with BoW
evaluate_model(nb_bow.best_estimator_, X_test_bow, y_test, "Naive Bayes with BoW")

# SVM with TF-IDF
evaluate_model(svm_tfidf.best_estimator_, X_test_tfidf, y_test, "SVM with TF-IDF")

# Naive Bayes with TF-IDF
evaluate_model(nb_tfidf.best_estimator_, X_test_tfidf, y_test, "Naive Bayes with TF-IDF")

tweet = input("Enter a tweet to predict its sentiment: ")

# Clean the input tweet
cleaned_tweet = custom_standardization(tweet)

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
vectorizer.fit(X_train)

# Vectorize the input tweet
tweet_vectorized = vectorizer.transform([cleaned_tweet])
# Predict the sentiment
model = svm_bow.best_estimator_  # Assign your trained model to the 'model' variable

prediction = model.predict(tweet_vectorized)

# Output the prediction
sentiment = "Positive" if prediction == 1 else "Negative"
print(f"Predicted sentiment: {sentiment}")